using Test
import Backprop: Jacobi
import Backprop.Jacobi: Tensor as Tensor, ⊙ as ⊙

@testset "Sigmoid-based functions" begin
    a = Tensor([-1.2682 0.4133 2.2894 0.7821;
         1.2522 0.6999 -0.4514 -1.0327;
        -0.0326 1.6305  1.1334  0.2879], requires_grad=true)
    b = Tensor([-0.0991 -0.1337 -1.1308;
        -1.0464  0.0141 -0.1314;
         0.4380 -1.0526  0.1453;
         1.6421 -2.6873  1.4708], requires_grad=true)
    c = Tensor([1.5517 -0.9088 0.8124;
        1.3773 -0.6192  2.0024;
        0.7790 -0.1417 -0.7258], requires_grad=true)
    
    t = Jacobi.sigmoid(a * b + c)
    Jacobi.backward!(t)
    @test t ≈ Tensor([0.9716 0.0052 0.9753;
        0.2022 0.9223 0.2516;
        0.5113 0.1110 0.4219]) atol=1e-3 
    @test a.grad ≈ Tensor([-0.0307 -0.0320 0.0101 0.0668;
        -0.2385 -0.1925 0.0226 0.3492;
        -0.3137 -0.2921 0.0411 0.5040]) atol=1e-3
    @test b.grad ≈ Tensor([0.1588 0.0799 0.1973;
         0.5317  0.2132  0.5394;
         0.2736  0.0914  0.2466;
        -0.0731 -0.0416 -0.1054]) atol=1e-3
    @test c.grad ≈ Tensor([0.0276 0.0052 0.0241;
        0.1613 0.0717 0.1883;
        0.2499 0.0986 0.2439]) atol=1e-3
    Jacobi.clear_grads(t)

    t = Jacobi.silu(a * b + c)
    Jacobi.backward!(t)
    @test t ≈ Tensor([3.4316 -0.0275 3.5843;
        -0.2775  2.2813 -0.2743;
         0.0231 -0.2309 -0.1329]) atol=1e-3 
    @test a.grad ≈ Tensor([-1.3060 -1.2588 0.6461 3.3798;
        -0.1975  0.0295 -1.1591 -2.9184;
        -0.4294 -0.5935  0.3783  1.6192]) atol=1e-3
    @test b.grad ≈ Tensor([-1.3970  1.4081 -1.3024;
        1.2805  0.6067 1.0347;
        3.0486 -0.6539 2.8058;
        1.0065 -1.1800 0.8835]) atol=1e-3
    @test c.grad ≈ Tensor([1.0691 -0.0221 1.0639;
        -0.0192  1.0996 0.0464;
         0.5226 -0.0943 0.3450]) atol=1e-3
    Jacobi.clear_grads(t)

    t = Jacobi.quick_gelu(a * b + c)
    Jacobi.backward!(t)
    @test t ≈ Tensor([3.5233e+00 -6.9625e-04 3.6681e+00;
        -1.2102e-01  2.4374e+00 -1.4744e-01;
         2.3504e-02 -5.8565e-02 -1.1628e-01]) atol=1e-3 
    @test a.grad ≈ Tensor([-1.2423 -1.1919 0.5912 3.1506;
        -0.0376  0.1298 -1.1566 -3.0949;
        -0.3203 -0.5965  0.3437  1.4281]) atol=1e-3
    @test b.grad ≈ Tensor([-1.4260 1.3134 -1.3913;
        1.2266  0.6196 0.7584;
        2.9727 -0.5525 2.6261;
        1.0496 -1.1009 0.9447]) atol=1e-3
    @test c.grad ≈ Tensor([1.0122 -0.0011 1.0101;
        -0.0997  1.0461 -0.0817;
         0.5385 -0.0687  0.2442]) atol=1e-3
    Jacobi.clear_grads(t)
end

@testset "RELU-based functions" begin
    a = Tensor([0.7311 -1.0689 1.2335 2.3177 0.5461;
        -0.1288 1.4017 0.4320 0.4303 -0.1202;
        0.9779 0.2056 2.1819 2.6843 -0.6234], requires_grad=true)
    b = Tensor([0.0149 0.6035 0.7277 -0.0278;
        0.0572  0.3283  0.1341 -1.4935;
        -0.8003 -0.2744  0.5300 -1.9165;
        0.5279 -0.3262  0.1307  1.9315;
        0.9235 -1.2887 -1.1083  0.3823], requires_grad=true)
    c = Tensor([ 2.4098 1.1163 -1.1256 -0.0528;
        -1.0203  1.4390 -0.6615 1.3957;
        0.9772 -0.4081 -0.5361 0.8900], requires_grad=true)

    t = Jacobi.elu(a * b + c)
    Jacobi.backward!(t)
    @test t ≈ Tensor([3.1002 -0.4466 -0.3199 3.8447;
        -0.6901  1.7174 -0.1383 -0.5214;
         0.0987 -0.3439  2.4012  1.3205]) atol=1e-3 
    @test a.grad ≈ Tensor([0.8160 -1.1634 -2.5082 2.3678 -0.1612;
        1.2219 -0.2532 -0.9829 0.8744 -1.7746;
        1.1108 -1.0868 -2.3668 2.3761 -0.6481]) atol=1e-3
    @test b.grad ≈ Tensor([1.6691 0.9174 1.3642 1.6474;
        -0.4290  0.9451  0.6865 -0.1925;
         3.5493  2.5462  3.3931  3.6222;
         5.1353  3.4742  4.6314  5.2079;
        -0.1145 -0.2270 -0.3556 -0.1348]) atol=1e-3
    @test c.grad ≈ Tensor([1.0000 0.5534 0.6801 1.0000;
        0.3099 1.0000 0.8617 0.4786;
        1.0000 0.6561 1.0000 1.0000]) atol=1e-3
    Jacobi.clear_grads(t)

    t = Jacobi.relu6(a * b + c)
    Jacobi.backward!(t)
    @test t ≈ Tensor([3.1002 0.0000 0.0000 3.8447;
        0.0000 1.7174 0.0000 0.0000;
        0.0987 0.0000 2.4012 1.3205]) atol=1e-3 
    @test a.grad ≈ Tensor([-0.0129 -1.4363 -2.7168 2.4594 1.3058;
        0.6035  0.3283 -0.2744 -0.3262 -1.2887;
        0.7148 -1.3022 -2.1868  2.5901  0.1975]) atol=1e-3
    @test b.grad ≈ Tensor([1.7090 -0.1288 0.9779 1.7090;
        -0.8633  1.4017  0.2056 -0.8633;
         3.4154  0.4320  2.1819  3.4154;
         5.0020  0.4303  2.6843  5.0020;
        -0.0773 -0.1202 -0.6234 -0.0773]) atol=1e-3
    @test c.grad ≈ Tensor([1. 0. 0. 1.;
        0. 1. 0. 0.;
        1. 0. 1. 1.]) atol=1e-3
    Jacobi.clear_grads(t)

    t = Jacobi.hard_silu(a * b + c)
    Jacobi.backward!(t)
    @test t ≈ Tensor([3.1002 -0.2375 -0.1680 3.8447;
        -0.3570  1.3503 -0.0707 -0.2780;
         0.0510 -0.1811  2.1616  0.9509]) atol=1e-3 
    @test a.grad ≈ Tensor([0.4402 -1.2871 -2.6030 2.4092 0.5039;
        0.9695  0.0389 -0.6307 0.2581 -1.6829;
        1.1451 -1.0813 -1.6378 2.1500 -1.0530]) atol=1e-3
    @test b.grad ≈ Tensor([1.2381 0.4348 1.4853 1.6177;
        -0.8059  1.2536  0.5016 -0.5191;
         2.4435  1.6213  3.4902  3.3948;
         3.7953  2.1283  4.5455  4.9509;
         0.2007 -0.1877 -0.6619 -0.0706]) atol=1e-3
    @test c.grad ≈ Tensor([1.0000 0.3028 0.3715 1.0000;
        0.1095 1.0725 0.4504 0.2544;
        0.5329 0.3595 1.3004 0.9402]) atol=1e-3
    Jacobi.clear_grads(t)

    t = Jacobi.leaky_relu(a * b + c)
    Jacobi.backward!(t)
    @test t ≈ Tensor([3.1002e+00 -5.9167e-03 -3.8548e-03 3.8447e+00;
        -1.1716e-02  1.7174e+00 -1.4884e-03 -7.3691e-03;
         9.8688e-02 -4.2140e-03  2.4012e+00  1.3205e+00]) atol=1e-3 
    @test a.grad ≈ Tensor([4.1200e-04 -1.4317e+00 -2.7142e+00 2.4574e+00 1.2818e+00;
        6.1065e-01  3.1528e-01 -2.9627e-01 -3.0030e-01 -1.2867e+00;
        7.2084e-01 -1.2989e+00 -2.1895e+00  2.5868e+00  1.8461e-01]) atol=1e-3
    @test b.grad ≈ Tensor([1.7077 -0.1117 0.9839 1.7077;
        -0.8493  1.3931  0.2089 -0.8493;
         3.4197  0.4662  2.1986  3.4197;
         5.0063  0.4803  2.7118  5.0063;
        -0.0785 -0.1210 -0.6191 -0.0785]) atol=1e-3
    @test c.grad ≈ Tensor([1.0000 0.0100 0.0100 1.0000;
        0.0100 1.0000 0.0100 0.0100;
        1.0000 0.0100 1.0000 1.0000]) atol=1e-3
    Jacobi.clear_grads(t)
end

@testset "Miscellaneous functions" begin
    a = Tensor([-0.9394 -0.8439 -1.0362;
        -0.6660  2.5343 0.7313;
        -0.3722 -1.2267 0.7689;
         0.9241 -0.3779 0.6887], requires_grad=true)
    b = Tensor([0.0258 -2.2431;
        1.4086 0.5099;
        0.4108 0.5086], requires_grad=true)
    c = Tensor([0.5589 0.9886;
         0.7804 -0.3453;
         1.1314 -0.3680;
        -0.6872  1.2085], requires_grad=true)
    
    t = Jacobi.gelu(a * b + c)
    Jacobi.backward!(t)
    @test t ≈ Tensor([-0.1515 2.1039;
         4.6334  2.8063;
        -0.1120  0.1376;
        -0.1650 -0.1696]) atol=1e-3 
    @test a.grad ≈ Tensor([-2.4056 0.4054 0.5038;
        -2.2596  1.9281  0.9290;
        -1.5230  0.7349  0.4598;
        -0.0474 -0.0727 -0.0138]) atol=1e-2
    @test b.grad ≈ Tensor([-0.7289 -1.9199;
        2.3039 0.8335;
        1.0056 0.1736]) atol=1e-3
    @test c.grad ≈ Tensor([-0.1000 1.0713;
         1.0000 1.0189;
         0.2748 0.6822;
        -0.0590 0.0205]) atol=1e-3
    Jacobi.clear_grads(t)

    t = Jacobi.softplus(a * b + c)
    Jacobi.backward!(t)
    @test t ≈ Tensor([0.2924 2.2498;
        4.6431 2.8711;
        0.5585 0.8161;
        0.3375 0.4009]) atol=1e-3 
    @test a.grad ≈ Tensor([-2.0001 0.8133 0.5591;
        -2.0905 1.8761 0.8866;
        -1.2403 0.8872 0.4595;
        -0.7335 0.5719 0.2857]) atol=1e-3
    @test b.grad ≈ Tensor([-0.7924 -1.3710;
        1.6627 0.8267;
        0.9878 0.4193]) atol=1e-3
    @test c.grad ≈ Tensor([0.2536 0.8946;
        0.9904 0.9434;
        0.4279 0.5579;
        0.2864 0.3303]) atol=1e-3
    Jacobi.clear_grads(t)

    t = Jacobi.mish(a * b + c)
    Jacobi.backward!(t)
    @test t ≈ Tensor([-0.3070 2.0914;
         4.6326  2.7948;
        -0.1471  0.1564;
        -0.2969 -0.2691]) atol=1e-3 
    @test a.grad ≈ Tensor([-2.3795 0.5872 0.5532;
        -2.2788 1.9346 0.9340;
        -1.6579 0.9633 0.5486;
        -0.4040 0.2212 0.1297]) atol=1e-3
    @test b.grad ≈ Tensor([-0.7676 -1.7906;
        1.9674 0.7273;
        1.0802 0.3485]) atol=1e-3
    @test c.grad ≈ Tensor([0.0327 1.0612;
        1.0015 1.0274;
        0.4146 0.7439;
        0.0914 0.1811]) atol=1e-3
    Jacobi.clear_grads(t)

    t = Jacobi.softsign(a * b + c)
    Jacobi.backward!(t)
    @test t ≈ Tensor([-0.5192 0.6814;
         0.8225  0.7377;
        -0.2250  0.1886;
        -0.4772 -0.4141]) atol=1e-3 
    @test a.grad ≈ Tensor([-0.2218 0.3774 0.1466;
        -0.1535 0.0795 0.0479;
        -1.4613 1.1818 0.5816;
        -0.7630 0.5600 0.2869]) atol=1e-3
    @test b.grad ≈ Tensor([-0.2092 -0.0690;
        -0.9554 -0.8487;
         0.4336  0.6877]) atol=1e-3
    @test c.grad ≈ Tensor([0.2312 0.1015;
        0.0315 0.0688;
        0.6007 0.6584;
        0.2733 0.3433]) atol=1e-3
    Jacobi.clear_grads(t)
end